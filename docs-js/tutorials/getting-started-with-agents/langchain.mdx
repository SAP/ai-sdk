---
id: langchain
title: Getting Started with Agents using LangChain
sidebar_label: LangChain
description: Building a Travel Itinerary Assistant with LangGraph and LangChain Client
keywords:
  - tutorial
  - agents
  - langgraph
  - langchain
  - openai
  - orchestration
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';

# Introduction

In this tutorial, you'll build a simple travel assistant agent using a LangChain Client, either (`OrchestrationClient`) or (`AzureOpenAiChatClient`) from the `@sap-ai-sdk/langchain` package alongside `LangGraph`.
The agent creates an itinerary based on current weather conditions and supports follow-up queries.

The assistant performs these steps:

- Accepts a user travel request.
- Calls a weather tool to fetch the current weather.
- Creates a weather-aware 3-item itinerary.
- Recommends restaurants upon request.
- Asks for human input (via interruption) when confirmation is required.

# Prerequisites

Refer to prerequisites outlined [here](../../overview-cloud-sdk-for-ai-js#prerequisites).

This tutorial assumes you have a basic understanding of TypeScript, LLMs and LangChain concepts.

## Installation

Install the following dependencies:

```bash
npm install @sap-ai-sdk/langchain langchain @langchain/langgraph @langchain/core zod
```

## Define Mock Tools

Define three tools:

- `get_weather` for returning weather by city
- `get_restaurants` for city-based restaurant recommendations
- `humanAssistanceTool` to trigger a human-in-the-loop question when LLM needs feedback.

:::info
This example uses mocked data for simplicity.
Replace the tools' implementation with real API calls to integrate live data.
:::

```ts
import { tool } from '@langchain/core/tools';
import { z } from 'zod';

const mockWeatherData: Record<
  string,
  { temperature: string; condition: string }
> = {
  paris: { temperature: '22°C', condition: 'Mild and pleasant' },
  reykjavik: { temperature: '3°C', condition: 'Cold and windy' }
};

const mockRestaurantData: Record<string, string[]> = {
  paris: ['Le Comptoir du Relais', "L'As du Fallafel", 'Breizh Café'],
  reykjavik: ['Dill Restaurant', 'Fish Market', 'Grillmarkaðurinn']
};

const getWeatherTool = tool(
  async ({ city }) => {
    const weather = mockWeatherData[city.toLowerCase()];
    return weather
      ? `Current weather in ${city}: ${weather.temperature}, ${weather.condition}`
      : `Weather data not available for ${city}.`;
  },
  {
    name: 'get_weather',
    description: 'Get current weather information for a city',
    schema: z.object({ city: z.string().describe('The city name') })
  }
);

const getRestaurantsTool = tool(
  async ({ city }) => {
    const restaurants = mockRestaurantData[city.toLowerCase()];
    return restaurants
      ? `Popular restaurants in ${city}: ${restaurants.join(', ')}`
      : `No restaurant data available for ${city}.`;
  },
  {
    name: 'get_restaurants',
    description: 'Get restaurant recommendations for a city',
    schema: z.object({ city: z.string().describe('The city name') })
  }
);

const humanAssistanceTool = tool(
  async ({ question }) => {
    return 'Human input requested';
  },
  {
    name: 'humanAssistanceTool',
    description:
      'Ask for human input when you need feedback or confirmation on itinerary suggestions',
    schema: z.object({
      question: z.string().describe('The question to ask the human')
    })
  }
);
```

## Setup the Agent

Initialize the client and add tools using the `buildTools()` method.
Set up a `ToolNode` for routing tool calls.

<Tabs 
  groupId="client"
  defaultValue="orchestration"
  values={[
    {label: "Orchestration Client", value: "orchestration"},
    {label: "OpenAI Client", value: "openai"}
  ]}>
<TabItem value="openai">

```ts
import { AzureOpenAiChatClient } from '@sap-ai-sdk/langchain';
import { ToolNode } from '@langchain/langgraph/prebuilt';

const tools = [getWeatherTool, getRestaurantsTool, humanAssistanceTool];

const model = new AzureOpenAiChatClient({
  modelName: 'gpt-4o',
  temperature: 0.7
}).bindTools(tools);

const toolNode = new ToolNode(tools);
```

</TabItem>
<TabItem value="orchestration">

```ts
import { OrchestrationClient } from '@sap-ai-sdk/langchain';
import { ToolNode } from '@langchain/langgraph/prebuilt';

const tools = [getWeatherTool, getRestaurantsTool];

const model = new OrchestrationClient({
  modelName: 'gpt-4o',
  temperature: 0.7
}).bindTools(tools);

const toolNode = new ToolNode(tools);
```

</TabItem>
</Tabs>

## Define Agent Logic and LangGraph Flow

Configure a `StateGraph` that routes between `agent` → `tool` → `agent`, and ends when no more tool calls are needed.

```ts
import {
  StateGraph,
  MessagesAnnotation,
  MemorySaver,
  START,
  interrupt,
  END
} from '@langchain/langgraph';
import { AIMessage, ToolMessage } from '@langchain/core/messages';

function shouldContinue({ messages }: typeof MessagesAnnotation.State) {
  const lastMessage = messages[messages.length - 1] as AIMessage;

  if (!lastMessage.tool_calls?.length) return END;

  if (lastMessage.tool_calls[0].name === 'humanAssistanceTool') {
    console.log('--- ASKING HUMAN ---');
    return 'askHuman';
  }

  return 'tools';
}

function askHuman({ messages }: typeof MessagesAnnotation.State) {
  const lastMessage = messages[messages.length - 1] as AIMessage;
  const toolCall = lastMessage.tool_calls?.[0];
  const toolCallId = toolCall?.id;
  const question = toolCall?.args?.question;

  const humanResponse = interrupt(`${question}`);
  const newToolMessage = new ToolMessage({
    tool_call_id: toolCallId!,
    content: humanResponse
  });

  return { messages: [newToolMessage] };
}

async function callModel(state: typeof MessagesAnnotation.State) {
  const response = await model.invoke(state.messages);
  return { messages: [response] };
}

const workflow = new StateGraph(MessagesAnnotation)
  .addNode('agent', callModel)
  .addNode('tools', toolNode)
  .addNode('askHuman', askHuman)
  .addConditionalEdges('agent', shouldContinue)
  .addEdge('tools', 'agent')
  .addEdge('askHuman', 'agent')
  .addEdge(START, 'agent');

const memory = new MemorySaver();
const app = workflow.compile({ checkpointer: memory });
```

:::note
Using [`MemorySaver`](https://langchain-ai.github.io/langgraphjs/how-tos/persistence/?h=memory+saver#add-memory) allows sharing context across multiple interactions in a single conversational thread (`conv-1`).
In this example, it enables the agent to remember the previously mentioned city.
:::

## Run the assistant

The following example simulates a full interaction, with detailed console output at each step:

1. The assistant receives an initial itinerary request.
2. It fetches the current weather for the destination city before creating the plan.
3. It asks the user for feedback and, if requested, suggests an updated itinerary.
4. It handles a follow-up request for restaurant recommendations in the same city.

```ts
import { HumanMessage, SystemMessage } from '@langchain/core/messages';
import { Command } from '@langchain/langgraph';

const config = { configurable: { thread_id: 'conv-1' } };

export async function runTravelAssistant() {
    // Initial system prompt and user message
    let messages = [
      new SystemMessage(
        `You are a helpful travel assistant. Create a short, practical 3-item itinerary based on the city weather.
        After you present a complete 3-item itinerary AND ask if they want adjustments, you MUST also call the humanAssistanceTool. Do this consistently for both initial and modified itineraries.`
      ),
      new HumanMessage(
        "I'm traveling to Paris. Can you help me prepare an itinerary?"
      )
    ];

    try {
      let response = await app.invoke({ messages }, config);

      console.log('Assistant:', response.messages[response.messages.length - 1].content);
      console.log('Assistant:', (response.messages[response.messages.length - 1] as AIMessage).tool_calls);
```

```console
[
  {
    id: 'call_VhqcEP76hN7DggRVWrrjMB7b',
    name: 'humanAssistanceTool',
    args: {
      question: 'Do you have any feedback or need adjustments on the suggested itinerary for Paris?'
    },
    type: 'tool_call'
  }
]
```

After calling the `humanAssistanceTool`, the graph pauses at the `askHuman` node and waits for the human's response.

```ts
console.log('next: ', (await app.getState(config)).next);
```

```bash
next:  [ 'askHuman' ]
```

Provide the response by invoking the graph with a `new Command({ resume: <follow_up_question>" })` input:

```ts
  response = await app.invoke(new Command({ resume: 'Can you suggest something more outdorsy?' }), config);

  console.log('Assistant:', response.messages[response.messages.length - 1].content);
  console.log("next: ", (await app.getState(config)).next);

  // Continue with restaurant request
  response = await app.invoke(new Command({ resume: 'Great! Can you also recommend some restaurants?' }), config);

  console.log('Assistant:', response.messages[response.messages.length - 1].content);

  } catch (error) {
    console.error('Error:', error);
  }
}
```

## Summary

This tutorial demonstrates how to implement a simple agent workflow using:

- The `AzureOpenAiChatClient` with `bindTools()` method to integrate external capabilities.
- Defining a LangGraph `StateGraph` for branching logic and conditional tool use.
- Interrupting the flow with a human tool via `interrupt()` and handling it inside the graph.
- Maintaining context across interactions with `MemorySaver`.

Explore additional capabilities and patterns in [LangGraph documentation](https://langchain-ai.github.io/langgraphjs/tutorials/quickstart/).
