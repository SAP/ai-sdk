---
id: openai
title: OpenAI Integration
hide_title: false
hide_table_of_contents: false
description: How to use the SAP Cloud SDK for AI to perform tasks using OpenAI models deployed on SAP AI Core.
keywords:
  - sap
  - cloud
  - sdk
  - ai
  - langchain
  - openai
---

The `@sap-ai-sdk/langchain` packages provides `AzureOpenAiChatClient` and `AzureOpenAiEmbeddingClient` clients for LangChain integration with Azure OpenAI.

## Client Initialization

Both clients reuse the Azure OpenAI clients from `@sap-ai-sdk/foundation-models` and implement the [LangChain's interface](https://js.langchain.com/docs/introduction).
Therefore, the client initialization combines the configuration of the foundation model and LangChain options.

Similar to the foundation model clients, the `AzureOpenAiChatClient` and `AzureOpenAiEmbeddingClient` LangChain clients can be initialized with model name and, by default, the latest model version.

```ts
import {
  AzureOpenAiChatClient,
  AzureOpenAiEmbeddingClient
} from '@sap-ai-sdk/langchain';

const chatClient = new AzureOpenAiChatClient({ modelName: 'gpt-4o' });
const embeddingClient = new AzureOpenAiEmbeddingClient({
  modelName: 'text-embedding-3-small'
});
```

Optionally, you can also specify model version, resource group and other parameters such as `max_tokens`, `temperature` and many more.

:::note
Providing a deployment ID is currently not supported.
:::

### Custom Destination

When initializing the client, it is possible to provide a custom destination for your SAP AI Core instance.

```ts
const chatClient = new AzureOpenAiChatClient(
  {
    modelName: 'gpt-4o'
  },
  {
    destinationName: 'my-destination'
  }
);
```

By default, the fetched destination is cached.
To disable caching, set the `useCache` parameter to `false` together with the `destinationName` parameter.

For more information about configuring a destination, refer to the [Using a Destination](connecting-to-ai-core#using-a-destination) section.

## Chat Completion

The `AzureOpenAiChatClient` allows interaction with Azure OpenAI chat models on SAP AI Core.
Pass a prompt to invoke the client.

```ts
const response = await chatClient.invoke("What's the capital of France?");
```

Below is an advanced example demonstrating the usage of templating and output parsing.

```ts
import { AzureOpenAiChatClient } from '@sap-ai-sdk/langchain';
import { StringOutputParser } from '@langchain/core/output_parsers';
import { ChatPromptTemplate } from '@langchain/core/prompts';

// Initialize the client
const chatClient = new AzureOpenAiChatClient({ modelName: 'gpt-35-turbo' });

// Create a prompt template
const promptTemplate = ChatPromptTemplate.fromMessages([
  ['system', 'Answer the following in {language}:'],
  ['user', '{text}']
]);

// Create an output parser
const parser = new StringOutputParser();

// Chain together template, client and parser
const llmChain = promptTemplate.pipe(chatClient).pipe(parser);

// Invoke the chain
return llmChain.invoke({
  language: 'german',
  text: 'What is the capital of France?'
});
```

## Embedding

The `AzureOpenAiEmbeddingClient` allows embedding text or document chunks (represented as arrays of strings).
While the embedding client can be used standalone, it is typically combined with other LangChain utilities, such as a text splitter for preprocessing and a vector store for storing and retrieving relevant embeddings.
For a complete example of how to implement RAG with the LangChain client, refer to the [sample code](https://github.com/SAP/ai-sdk-js/blob/main/sample-code/src/langchain-azure-openai.ts).

### Embed Text

```ts
const embeddedText = await embeddingClient.embedQuery(
  'Paris is the capital of France.'
);
```

### Embed Document Chunks

```ts
const embeddedDocuments = await embeddingClient.embedDocuments([
  'Page 1: Paris is the capital of France.',
  'Page 2: It is a beautiful city.'
]);
```

### Preprocess, embed, and store documents

```ts
// Create a text splitter and split the document
const textSplitter = new RecursiveCharacterTextSplitter({
  chunkSize: 2000,
  chunkOverlap: 200
});
const splits = await textSplitter.splitDocuments(docs);

// Initialize the embedding client
const embeddingClient = new AzureOpenAiEmbeddingClient({
  modelName: 'text-embedding-3-small'
});

// Create a vector store from the document
const vectorStore = await MemoryVectorStore.fromDocuments(
  splits,
  embeddingClient
);

// Create a retriever for the vector store
const retriever = vectorStore.asRetriever();
```

## Resilience

Use LangChain options such as `maxRetries` and `timeout` to provide resilience.

### Retry

By default, LangChain client retries up to six times with exponential delay.
To modify this behavior, set the `maxRetries` option during the client initialization.

```ts
const client = new AzureOpenAiChatClient(
  { modelName: 'gpt-4o' },
  {
    maxRetries: 0
  }
);
```

### Timeout

By default, no timeout is set in the client.
To limit the maximum duration for the entire request including retries, specify a timeout in milliseconds when calling the `invoke` method.

```ts
const response = await client.invoke(messageHistory, { timeout: 10000 });
```
